{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can Language Models Solve Graph Problems in Natural Language?\n",
    "\n",
    "#### Sarbpreet Ghotra & Sameer Ladha\n",
    "\n",
    "#### Emails: sarbpreet.ghotra@torontomu.ca & sameer.ladha@torontomu.ca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction:**\n",
    "The landscape of artificial intelligence (AI) is rapidly evolving with the development of Large Language Models (LLMs), which have demonstrated remarkable capabilities in mimicking human-like text generation and versatility across various applications, including complex problem-solving. The paper \"Can Language Models Solve Graph Problems in Natural Language?\" by Wang et al. explores the potential of LLMs to handle graph-related tasks, traditionally managed with specialized algorithms. This research delves into how well LLMs can interpret, map, and solve graph-based problems using natural language, potentially transforming problem-solving by making it more accessible to a broader audience.\n",
    "#### **Problem Description:**\n",
    "\n",
    "The main problem the study aims to investigate is whether large language models (LLMs) can effectively solve structured graph-based problems when presented in natural language, moving beyond simple text generation to more complex, problem-solving tasks. This research addresses a significant challenge in applying AI to real-world problems, as graphs are fundamental in various domains such as social networks and transportation systems. The goal is to make advanced problem-solving accessible to a broader audience without the need for specialized algorithmic knowledge.   \n",
    "\n",
    "This research aims to discover if LLMs can process graph descriptions, map them to accurate conceptual models, and solve graph reasoning tasks using natural language. The researchers main goal was to examine whether LLMs can:\n",
    "- Grasp different natural language descriptions of graphs, identifying nodes, edges, and other elements.\n",
    "- Translate these descriptions into conceptual graph representations that mirror the actual structure and relationships.\n",
    "- Use algorithmic reasoning on these models to address common graph problems, like finding the shortest path or calculating maximum flow.\n",
    "- Understand how LLMs' problem-solving abilities change with task complexity and identify their limitations.\n",
    "\n",
    "#### **Context of the Problem**\n",
    "\n",
    "The exploration into Large Language Models (LLMs) solving graph problems in natural language relies on the foundation of increasing complexity in data structures and the need for more intuitive ways of interacting with such data. Graphs, by their very nature, are a pervasive data structure used across a multitude of disciplines to represent complex relationships and interactions (Chen et. al., 2024). Traditionally, this has required not only a deep understanding of graph theory but also the ability to apply sophisticated algorithmic techniques, often necessitating specialized computational tools and programming expertise. The idea of utilizing LLMs to interpret and solve these problems through natural language represents a shift in how we approach complex problem-solving. This shift is motivated by the goals of accessibility and efficiency (Wang et. al., 2023). On one hand, there's a growing demand for making advanced computational capabilities accessible to a broader audience without the prerequisite of technical expertise in programming or mathematics. On the other hand, the efficiency of solving complex problems through intuitive, natural language interactions presents a compelling case for integrating LLMs into various domains where graph-based problems are prevalent.\n",
    "#### **Limitations of Other Approaches**\n",
    "\n",
    "\n",
    "Current studies highlight several limitations in applying large language models (LLMs) to graph reasoning tasks. One significant issue is LLMs' inability to perform precise mathematical or complex reasoning, as they were originally designed for text processing, not for explicit problem-solving in structured domains like graphs. Additionally, the research identifies a need for more diverse datasets and notes the diminishing returns of advanced prompting techniques as task complexity increases, underscoring the challenge in enhancing LLMs' effectiveness for complex graph-based problem-solving using natural language. Another notable limitation is LLMs' likeliehood to untrue correlations within problem settings.Despite introducing promising prompting strategies like Build-a-Graph and Algorithmic Prompting, Wang et al. (2023) acknowledge that the most complex graph reasoning tasks remain unsolved. Wang et al. (2023) also saw diminishing returns of advanced prompting techniques like chain-of-thought (COT), least-to-most (LTM), and self-consistency (SC) as the complexity of graph reasoning tasks increases. This highlights the ongoing challenge in enabling LLMs to effectively tackle graph problems through natural language, necessitating further research and innovation in this area.\n",
    "\n",
    "\n",
    "#### **Solution**\n",
    "##### **NLGraph Benchmark**\n",
    "- Comprehensive Testbed: The NLGraph benchmark is a collection of graph-based problems designed in natural language. It covers a wide range of graph reasoning tasks, from simple connectivity queries to complex graph neural network simulations, resulting in a platform for evaluating LLMs' graph reasoning capabilities.\n",
    "- Varied Complexity: The benchmark includes problems of varying complexity, categorized into easy, medium, and hard subsets. This allows for a nuanced analysis of LLMs' performance across different levels of difficulty and task types.\n",
    "- Evaluation Metrics: Beyond binary correctness, the benchmark employs partial credit solutions and exact match accuracy as metrics, enabling a more detailed assessment of LLMs' reasoning processes and solutions.\n",
    "##### **Instruction-based Prompting Techniques**\n",
    "- Build-a-Graph Prompting (BAG): This technique involves instructing the LLM to first construct a mental model of the graph based on the textual description before attempting to solve the problem. It's aimed at encouraging the model to better visualize and understand the graph's structure, enhancing its reasoning ability.\n",
    "- Algorithmic Prompting: Here, the LLM is guided to consider algorithmic steps relevant to solving the specific graph problem at hand. By nudging the model to recall and apply algorithmic logic, this technique seeks to improve the accuracy and depth of the LLM's solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Background**\n",
    "| Reference                 | Explanation                                                                                                   | Dataset/Input | Weakness                                       |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------------------- | ------------- | ---------------------------------------------- |\n",
    "|(Zhang, 2023)       |Developing a framework named Graph-ToolFormer to enhance Large Language Models (LLMs) with the ability to reason about complex graph data|Graph Reasoning Prompt Dataset (made by researchers)| Exisitng LLM's cannot do percicise math or reasoning problems|\n",
    "|(Jin et. al., 2024) |Systematic review of scenarios and techniques related to the application of Large Language Models (LLMs) on graph structures |Various Datasets such as NL-Graph (this paper), LLMtoGraph and GUC| Future Direction indicates that more diverse datasets from other domains are needed|\n",
    "|(Huang et. al., 2022) |Explores whether LLMs can utilize their learned world knowledge to perform tasks in interactive environments, specifically by decomposing high-level tasks into actionable steps |VirtualHome Environment|VirtualHome cannot fully complete all tasks thus leading to lower evaluation scores|\n",
    "|(Chen et. al., 2024)|Investigates the potential of (LLMs) in graph machine learning, particularly for node classification tasks, using Graph Neural Networks (GNNs)|Ogbn-arxiv|Ground truth labels may present ambiguity, and the performance calculated based on them may not re- flect LLMs’ genuine capability|\n",
    "|(Wang et. al., 2023)|Explore whether LLMs can effectively process and reason about graph structures presented in natural language|Generated own dataset|Tasks in NLGraph benchmark are not complete|\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methodology**\n",
    "\n",
    "\n",
    "In this study we are evaluating the graph reasoning abilities of LLMs, particularly \"davinci-002\" as the original study used \"davinci-003\" as their default LLM (davinci-003 is now depreciated). We are evaluting the LLM across different types of graph problems using specific prompting techniques. This study's methodology introduces and tests a dual prompting system where, Build-a-Graph Prompting helps LLMs visualize and structure the problem contextually before solving it, while Algorithmic Prompting introduces algorithmic thinking by outlining steps relevant to the graph problem at hand. This approach is tested across various graph reasoning tasks such as hamilton path, cycles, and shortest paths to evaluate its effectiveness with accuracy. We will be comparing both techniques with standard Chain-of-Thought method. Both prompting techniques are designed to enhance the capabilities of LLMs by providing them with additional context and structured thinking paths. Below you will find more information about the different techniques and tasks.\n",
    "\n",
    "\n",
    "![Description of your image](Figs/Standard-BAG-Alg.png)  \n",
    "\n",
    "### **Build-a-Graph Prompting (BAG)**\n",
    "\n",
    "\n",
    "Build-a-Graph Prompting encourages LLMs to visually map out the graph's structure before addressing the query. With this technique we append a instruction such as \"Let's construct a graph with the nodes and edges first\" right after the textual description of the graph. The idea is that by first visualizing or conceptualizing the graph structure, the language model can better understand and prepare for the problem-solving process that follows. \n",
    "\n",
    "##### Example Prompt  \n",
    "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.  \n",
    "The nodes are numbered from 0 to 5, and the edges are: (3,4) (3,5) (1,0) (2,5) (2,0). Let's construct a graph with the nodes and edges first.  \n",
    "Q: Is there a cycle in this graph?  \n",
    "A: No, there is no cycle in this graph.\n",
    "\n",
    "---\n",
    "### **Algorithmic Prompting**\n",
    "\n",
    "\n",
    "Algorithmic Prompting, on the other hand, involves revisiting and reflecting on the algorithmic steps needed for solving a graph-related task. Before presenting the main problem, we introduce details about relevant algorithms, such as Depth-First Search (DFS) for finding paths or detecting cycles in graphs. The prompt would start with a line like \"We can use a Depth-First Search (DFS) algorithm to find the shortest path between two given nodes in an undirected graph.\"\n",
    "\n",
    "##### Example Prompt  \n",
    "To determine whether or not there is a cycle in an undirected graph, you can use a depth-first search algorithm to traverse the graph.  \n",
    "If the algorithm ever returns to a node it has already visited, then it has detected a cycle in the graph.  \n",
    "\n",
    "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.  \n",
    "The nodes are numbered from 0 to 5, and the edges are: (3,4) (3,5) (1,0) (2,5) (2,0)  \n",
    "Q: Is there a cycle in this graph?  \n",
    "A: No, there is no cycle in this graph.  \n",
    "\n",
    "---\n",
    "### **Chain-of-Thought (CoT)**\n",
    "\n",
    "Chain-of-Thought (CoT) is a prompting technique used to guide language models to generate intermediate reasoning steps before arriving at a final answer. This method helps the model articulate the thought process involved in solving a problem, potentially leading to more accurate and robust solutions.\n",
    "\n",
    "##### Example Prompt  \n",
    "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.  \n",
    "The nodes are numbered from 0 to 5, and the edges are: (0,1) (2,6) (2,0) (5,3) (1,4) (4,7) (3,7) (6,4)  \n",
    "Q: Is there a cycle in this graph?  \n",
    "A: The edges (0,1), (1,4), (6,4), (2,6), (2,0) form a cycle, so yes, there is a cycle in this graph.  \n",
    "\n",
    "![Description of your image](Figs/cyclesfig.png)  ![Description of your image](Figs/shortestpathfig.png)    ![Description of your image](Figs/hamiltonpathfig.png)\n",
    "\n",
    "## **Tasks**  \n",
    "### **Cycles Task**\n",
    "\n",
    "The study uses this task to test if language models can identify whether any cyclic dependencies exist in graph structures presented in natural language, which is fundamental for understanding complex systems and relationships. A cycle in an undirected graph is a sequence of vertices starting and ending at the same vertex, with each pair of consecutive vertices connected by an edge, and no vertex appearing twice.\n",
    "\n",
    "##### Example Prompt  \n",
    "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.  \n",
    "The nodes are numbered from 0 to 5, and the edges are: (3,4) (3,5) (1,0) (2,5) (2,0)  \n",
    "Q: Is there a cycle in this graph?  \n",
    "A: No, there is no cycle in this graph.  \n",
    "\n",
    "---\n",
    "### **Shortest Path Task**\n",
    "\n",
    "The Shortest Path task in graph theory involves finding the minimum path or minimum sum of weights between two nodes in a graph. It's a fundamental problem with applications in routing, navigation, and network optimization. For the Shortest Path, models were prompted to calculate paths based on edge weights. This task in the NLGraph benchmark checks if language models can find the most efficient connection or pathway between points in a graph when described in natural language, mimicking real-world routing problems.  \n",
    "\n",
    "##### Example Prompt  \n",
    "In an undirected graph, the nodes are numbered from 0 to 4, and the edges are:  \n",
    "an edge between node 0 and node 1 with weight 2,  \n",
    "an edge between node 1 and node 4 with weight 1,  \n",
    "an edge between node 0 and node 2 with weight 1,  \n",
    "an edge between node 2 and node 3 with weight 1,  \n",
    "an edge between node 3 and node 4 with weight 2.  \n",
    "Q: Give the shortest path from node 0 to node 4.  \n",
    "A: All the paths from node 0 to node 4 are:  \n",
    "0,1,4 with a total weight of 2 + 1 = 3,  \n",
    "0,2,3,4 with a total weight of 1 + 1 + 2 = 4.  \n",
    "The weight of path 0,1,4 is the smallest, so the shortest path from node 0 to node 4 is 0,1,4 with a total weight of 3.\n",
    "\n",
    "---\n",
    "### **Hamilton Path Task**\n",
    "\n",
    "For the Hamilton Path, the models were required to identify a path that visits all nodes exactly once. ​It is used in the benchmark to assess if language models can plan a route that visits each node of a graph exactly once when the graph is described in natural language, testing the models' planning and combinatorial optimization skills.  \n",
    "\n",
    "##### Example Prompt  \n",
    "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.  \n",
    "The nodes are numbered from 0 to 4, and the edges are: (4,2) (0,4) (4,3) (0,1) (0,2) (4,1) (2,3)  \n",
    "Q: Is there a path in this graph that visits every node exactly once? If yes, give the path. Note that in a path, adjacent nodes must be connected with edges.  \n",
    "A: Yes. We can start at node 0. As node 0 is connected with node 1, and node 1 is not visited, we can then visit node 1.  \n",
    "As node 1 is connected with node 4, and node 4 is not visited, we can then visit node 4.  \n",
    "As node 4 is connected with node 2, and node 2 is not visited, we can then visit node 2.  \n",
    "As node 2 is connected with node 3, and node 3 is not visited, we can then visit node 3.  \n",
    "Therefore, the path can be: 0,1,4,2,3.  \n",
    "\n",
    "---\n",
    "### **Task Script File Example**  \n",
    "Double click the \"Toggle Code\" button below to view the Cycle.py file that is used to evaluate the LLM. For the sake of clarity in this notebook, we will only be showing this specifc one here, however other task python files could be found in our repo, in the evaluation folder.  \n",
    "\n",
    "The cycle.py and other task scripts play a crucial role in our evaluation process by configuring and running experiments to assess the ability of various language models to identify cycles in undirected graphs. The script initializes by configuring command-line arguments for selecting the model, difficulty level, and prompting techniques. It uses these configurations to dynamically generate graph-related queries. Using networkx, a library for the creation, manipulation, and study of complex networks, the script constructs graph instances. It then formats these instances into natural language prompts that ask whether a cycle exists within the graph, tailored according to the specified prompting technique. Leveraging OpenAI's API, the script sends these prompts to the specified language model and retrieves the responses. Responses are collected, and the script determines the accuracy of each model's cycle detection based on the answers. Results, along with other relevant data, are logged to a file for later analysis. It processes graphs in batches to efficiently manage memory and computation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "function toggleCode() {\n",
    "    var x = document.getElementById(\"code_block\");\n",
    "    if (x.style.display === \"none\") {\n",
    "        x.style.display = \"block\";\n",
    "    } else {\n",
    "        x.style.display = \"none\";\n",
    "    }\n",
    "}\n",
    "</script>\n",
    "\n",
    "<button onclick=\"toggleCode()\">Toggle Code</button>\n",
    "\n",
    "<div id=\"code_block\" style=\"display:none\">\n",
    "    <code>\n",
    "\n",
    "    #CODE STARTS HERE\n",
    "    \n",
    "import openai\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # Handles retries for robust API calls\n",
    "\n",
    "# Define a list of models\n",
    "model_list = [\"text-davinci-003\",\"code-davinci-002\",\"gpt-3.5-turbo\",\"gpt-4\"]\n",
    "# Setting up command line arguments\n",
    "parser = argparse.ArgumentParser(description=\"cycle\")\n",
    "parser.add_argument('--model', type=str, default=\"text-davinci-003\", help='name of LM (default: text-davinci-003)')\n",
    "parser.add_argument('--mode', type=str, default=\"easy\", help='mode (default: easy)')\n",
    "parser.add_argument('--prompt', type=str, default=\"none\", help='prompting techniques (default: none)')\n",
    "parser.add_argument('--T', type=int, default=0, help='temperature (default: 0)')\n",
    "parser.add_argument('--token', type=int, default=400, help='max token (default: 400)')\n",
    "parser.add_argument('--SC', type=int, default=0, help='self-consistency (default: 0)')\n",
    "parser.add_argument('--SC_num', type=int, default=5, help='number of cases for self-consistency (default: 5)')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Assert to check that the prompt type is supported\n",
    "assert args.prompt in [\"CoT\", \"none\", \"0-CoT\", \"LTM\", \"PROGRAM\",\"k-shot\",\"Instruct\",\"Algorithm\", \"Recitation\",\"hard-CoT\",\"medium-CoT\"]\n",
    "\n",
    "# Function to construct the question about cycles in a graph in natural language format\n",
    "def translate(edge, n, args):\n",
    "    Q = ''\n",
    "    # Load different example prompts based on the type\n",
    "    if args.prompt in [\"CoT\", \"k-shot\", \"Instruct\", \"Algorithm\", \"Recitation\",\"hard-CoT\",\"medium-CoT\"]:\n",
    "        with open(\"NLGraph/cycle/prompt/\" + args.prompt + \"-prompt.txt\", \"r\") as f:\n",
    "            exemplar = f.read()\n",
    "        Q = Q + exemplar + \"\\n\\n\\n\"\n",
    "    # Construct the graph description\n",
    "    Q = Q + \"In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\\nThe nodes are numbered from 0 to \" + str(n-1)+\", and the edges are:\"\n",
    "    for i in range(len(edge)):\n",
    "        Q = Q + ' ('+str(edge[i][0])+','+str(edge[i][1])+')'\n",
    "    # Add specific instructional text based on the prompting type\n",
    "    if args.prompt == \"Instruct\":\n",
    "        Q = Q + \". Let's construct a graph with the nodes and edges first.\"\n",
    "    Q = Q + \"\\n\"\n",
    "    if args.prompt == \"Recitation\":\n",
    "        Q = Q + \"Q1: Are node \"+str(edge[0][0])+\" and node \" +str(edge[0][1])+\" connected with an edge?\\nA1: Yes.\\n\"\n",
    "        u = -1\n",
    "        for i in range(n):\n",
    "            if u != -1:\n",
    "                break\n",
    "            for j in range(n):\n",
    "                if [i,j] not in edge:\n",
    "                    u, v = i, j\n",
    "                    break\n",
    "        Q = Q + \"Q2: Are node \"+str(u)+\" and node \" +str(v)+\" connected with an edge?\\nA2: No.\\n\"\n",
    "    Q = Q + \"Q: Is there a cycle in this graph?\\nA:\"\n",
    "    # Add step-by-step analysis prompts\n",
    "    match args.prompt:\n",
    "        case \"0-CoT\":\n",
    "            Q = Q + \" Let's think step by step:\"\n",
    "        case \"LTM\":\n",
    "            Q = Q + \" Let's break down this problem:\"\n",
    "        case \"PROGRAM\":\n",
    "            Q = Q + \" Let's solve the problem by a Python program:\"\n",
    "    return Q\n",
    "\n",
    "# A retry mechanism to handle API requests robustly\n",
    "@retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(1000))\n",
    "def predict(Q, args):\n",
    "    input = Q\n",
    "    temperature = 0\n",
    "    if args.SC == 1:\n",
    "        temperature = 0.7\n",
    "    if 'gpt' in args.model:\n",
    "        Answer_list = []\n",
    "        for text in input:\n",
    "            response = openai.ChatCompletion.create(\n",
    "            model=args.model,\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=args.token,\n",
    "            )\n",
    "            Answer_list.append(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        return Answer_list\n",
    "    response = openai.Completion.create(\n",
    "    model=args.model,\n",
    "    prompt=input,\n",
    "    temperature=temperature,\n",
    "    max_tokens=args.token,\n",
    "    )\n",
    "    Answer_list = []\n",
    "    for i in range(len(input)):\n",
    "        Answer_list.append(response[\"choices\"][i][\"text\"])\n",
    "    return Answer_list\n",
    "\n",
    "# Logging results to files\n",
    "def log(Q, res, answer, args):\n",
    "    utc_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "    bj_dt = utc_dt.astimezone(timezone(timedelta(hours=8)))\n",
    "    time = bj_dt.now().strftime(\"%Y%m%d---%H-%M\")\n",
    "    newpath = 'log/cycle/'+args.model+'-'+args.mode+'-'+time+ '-' + args.prompt\n",
    "    if args.SC == 1:\n",
    "        newpath = newpath + \"+SC\"\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    newpath = newpath + \"/\"\n",
    "    np.save(newpath+\"res.npy\", res)\n",
    "    np.save(newpath+\"answer.npy\", answer)\n",
    "    with open(newpath+\"prompt.txt\",\"w\") as f:\n",
    "        f.write(Q)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Acc: \" + str(res.sum())+'/'+str(len(res)) + '\\n')\n",
    "        print(args, file=f)\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    # Ensure the OpenAI API key and organization are set\n",
    "    if 'OPENAI_API_KEY' in os.environ:\n",
    "        openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "    else:\n",
    "        raise Exception(\"Missing openai key!\")\n",
    "    if 'OPENAI_ORGANIZATION' in os.environ:\n",
    "        openai.organization = os.environ['OPENAI_ORGANIZATION']\n",
    "    res, answer = [], []\n",
    "    match args.mode:\n",
    "        case \"easy\":\n",
    "            g_num = 150\n",
    "        case \"medium\":\n",
    "            g_num = 600\n",
    "        case \"hard\":\n",
    "            g_num = 400\n",
    "    batch_num = 20\n",
    "    for i in tqdm(range((g_num + batch_num - 1) // batch_num)):\n",
    "        G_list, Q_list = [], []\n",
    "        for j in range(i*batch_num, min(g_num, (i+1)*batch_num)):\n",
    "            with open(\"NLgraph/cycle/graph/\"+args.mode+\"/standard/graph\"+str(j)+\".txt\",\"r\") as f:\n",
    "                n, m = [int(x) for x in next(f).split()]\n",
    "                edge = []\n",
    "                for line in f: # read rest of lines\n",
    "                    edge.append([int(x) for x in line.split()])\n",
    "                G = nx.Graph()\n",
    "                G.add_nodes_from(range(n))\n",
    "                for k in range(m):\n",
    "                    G.add_edge(edge[k][0], edge[k][1])\n",
    "                Q = translate(edge, n, args)\n",
    "                Q_list.append(Q)\n",
    "                G_list.append(G)\n",
    "        sc = 1\n",
    "        if args.SC == 1:\n",
    "            sc = args.SC_num\n",
    "        sc_list = []\n",
    "        for k in range(sc):\n",
    "            answer_list = predict(Q_list, args)\n",
    "            sc_list.append(answer_list)\n",
    "        for j in range(len(Q_list)):\n",
    "            vote = 0\n",
    "            for k in range(sc):\n",
    "                ans, G = sc_list[k][j].lower(), G_list[j]\n",
    "                answer.append(ans.lower())\n",
    "                result = 0\n",
    "                pos = max(ans.find(\"in this case\"), ans.find(\"after running the algorithm\"))\n",
    "                if pos == -1:\n",
    "                    pos = 0\n",
    "                p1 = ans.find(\"there is no cycle\")  # for codex\n",
    "                p2 = ans.find(\"there is a cycle\")  # for codex\n",
    "                p1 = 1000000 if p1 == -1 else p1\n",
    "                p2 = 1000000 if p2 == -1 else p2\n",
    "                idx = i * batch_num + j\n",
    "                if (idx*2 < g_num and p1 < p2) or (idx*2 > g_num and p2 < p1):\n",
    "                    vote += 1\n",
    "            if vote * 2 >= sc:\n",
    "                res.append(1)\n",
    "            else:\n",
    "                res.append(0)    \n",
    "    res = np.array(res)\n",
    "    answer = np.array(answer)\n",
    "    log(Q, res, answer, args)\n",
    "    print(res.sum())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    </code>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary dependencies by creating environment called NLGraph that should be able to run the code. Environment.yml is located in our repo. Make sure all neccessary execution and prompt files are downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda env create -n NLGraph -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your own openai api key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENAI_API_KEY='api-key-here'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%env OPENAI_API_KEY='api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the api key is set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'api-key-here'\n"
     ]
    }
   ],
   "source": [
    "!echo %OPENAI_API_KEY%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we start the evaluation process, starting with the Chain-of-Thought method, we make our way through the BAG and Algorithmic method. Then we gather the results from the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chain-of-Thought**  \n",
    "\n",
    "Now we test the Chain-of-Thought prompting method below for the different tasks and difficulties which acts as a baseline to compare with, against the proposed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = COT  \n",
    "##### Task = Cycle  \n",
    "##### Difficulty = Easy Medium Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:03<00:25,  3.66s/it]\n",
      " 25%|██▌       | 2/8 [00:06<00:20,  3.42s/it]\n",
      " 38%|███▊      | 3/8 [00:10<00:16,  3.32s/it]\n",
      " 50%|█████     | 4/8 [00:13<00:12,  3.24s/it]\n",
      " 62%|██████▎   | 5/8 [00:16<00:09,  3.21s/it]\n",
      " 75%|███████▌  | 6/8 [00:19<00:06,  3.22s/it]\n",
      " 88%|████████▊ | 7/8 [00:22<00:03,  3.21s/it]\n",
      "100%|██████████| 8/8 [00:25<00:00,  3.12s/it]\n",
      "100%|██████████| 8/8 [00:25<00:00,  3.22s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode easy --prompt CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\n",
      "  3%|▎         | 1/30 [00:04<01:57,  4.06s/it]\n",
      "  7%|▋         | 2/30 [00:07<01:40,  3.59s/it]\n",
      " 10%|█         | 3/30 [00:10<01:32,  3.42s/it]\n",
      " 13%|█▎        | 4/30 [00:13<01:27,  3.35s/it]\n",
      " 17%|█▋        | 5/30 [00:17<01:22,  3.31s/it]\n",
      " 20%|██        | 6/30 [00:20<01:19,  3.30s/it]\n",
      " 23%|██▎       | 7/30 [00:23<01:15,  3.29s/it]\n",
      " 27%|██▋       | 8/30 [00:26<01:11,  3.25s/it]\n",
      " 30%|███       | 9/30 [00:29<01:07,  3.23s/it]\n",
      " 33%|███▎      | 10/30 [00:33<01:04,  3.22s/it]\n",
      " 37%|███▋      | 11/30 [00:36<01:00,  3.21s/it]\n",
      " 40%|████      | 12/30 [00:39<00:58,  3.24s/it]\n",
      " 43%|████▎     | 13/30 [00:43<00:56,  3.31s/it]\n",
      " 47%|████▋     | 14/30 [00:46<00:52,  3.31s/it]\n",
      " 50%|█████     | 15/30 [00:49<00:49,  3.28s/it]\n",
      " 53%|█████▎    | 16/30 [00:52<00:45,  3.26s/it]\n",
      " 57%|█████▋    | 17/30 [00:56<00:42,  3.27s/it]\n",
      " 60%|██████    | 18/30 [00:59<00:39,  3.27s/it]\n",
      " 63%|██████▎   | 19/30 [01:02<00:35,  3.27s/it]\n",
      " 67%|██████▋   | 20/30 [01:05<00:32,  3.26s/it]\n",
      " 70%|███████   | 21/30 [01:09<00:29,  3.25s/it]\n",
      " 73%|███████▎  | 22/30 [01:12<00:26,  3.26s/it]\n",
      " 77%|███████▋  | 23/30 [01:15<00:22,  3.26s/it]\n",
      " 80%|████████  | 24/30 [01:18<00:19,  3.25s/it]\n",
      " 83%|████████▎ | 25/30 [01:22<00:16,  3.26s/it]\n",
      " 87%|████████▋ | 26/30 [01:25<00:12,  3.24s/it]\n",
      " 90%|█████████ | 27/30 [01:28<00:09,  3.27s/it]\n",
      " 93%|█████████▎| 28/30 [01:31<00:06,  3.27s/it]\n",
      " 97%|█████████▋| 29/30 [01:35<00:03,  3.26s/it]\n",
      "100%|██████████| 30/30 [01:38<00:00,  3.26s/it]\n",
      "100%|██████████| 30/30 [01:38<00:00,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode medium --prompt CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|▌         | 1/20 [00:03<01:08,  3.62s/it]\n",
      " 10%|█         | 2/20 [00:06<01:01,  3.43s/it]\n",
      " 15%|█▌        | 3/20 [00:09<00:55,  3.26s/it]\n",
      " 20%|██        | 4/20 [00:13<00:52,  3.28s/it]\n",
      " 25%|██▌       | 5/20 [00:16<00:49,  3.32s/it]\n",
      " 30%|███       | 6/20 [00:19<00:45,  3.27s/it]\n",
      " 35%|███▌      | 7/20 [00:23<00:42,  3.27s/it]\n",
      " 40%|████      | 8/20 [00:26<00:38,  3.21s/it]\n",
      " 45%|████▌     | 9/20 [00:29<00:35,  3.24s/it]\n",
      " 50%|█████     | 10/20 [00:32<00:32,  3.22s/it]\n",
      " 55%|█████▌    | 11/20 [00:36<00:29,  3.25s/it]\n",
      " 60%|██████    | 12/20 [00:39<00:25,  3.23s/it]\n",
      " 65%|██████▌   | 13/20 [00:42<00:22,  3.25s/it]\n",
      " 70%|███████   | 14/20 [00:46<00:20,  3.34s/it]\n",
      " 75%|███████▌  | 15/20 [00:49<00:16,  3.31s/it]\n",
      " 80%|████████  | 16/20 [00:53<00:13,  3.44s/it]\n",
      " 85%|████████▌ | 17/20 [00:56<00:10,  3.38s/it]\n",
      " 90%|█████████ | 18/20 [00:59<00:06,  3.38s/it]\n",
      " 95%|█████████▌| 19/20 [01:03<00:03,  3.64s/it]\n",
      "100%|██████████| 20/20 [01:08<00:00,  4.02s/it]\n",
      "100%|██████████| 20/20 [01:08<00:00,  3.44s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode hard --prompt CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = COT  \n",
    "##### Task = Shortest Path  \n",
    "##### Difficulty = Easy Hard  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\n",
      " 11%|█         | 1/9 [00:03<00:30,  3.87s/it]\n",
      " 22%|██▏       | 2/9 [00:07<00:25,  3.65s/it]\n",
      " 33%|███▎      | 3/9 [00:10<00:21,  3.58s/it]\n",
      " 44%|████▍     | 4/9 [00:14<00:17,  3.55s/it]\n",
      " 56%|█████▌    | 5/9 [00:18<00:14,  3.59s/it]\n",
      " 67%|██████▋   | 6/9 [00:21<00:10,  3.59s/it]\n",
      " 78%|███████▊  | 7/9 [00:25<00:07,  3.58s/it]\n",
      " 89%|████████▉ | 8/9 [00:28<00:03,  3.60s/it]\n",
      "100%|██████████| 9/9 [00:34<00:00,  4.29s/it]\n",
      "100%|██████████| 9/9 [00:34<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/shortest_path.py --model davinci-002 --mode easy --prompt CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:04<00:37,  4.14s/it]\n",
      " 20%|██        | 2/10 [00:07<00:29,  3.74s/it]\n",
      " 30%|███       | 3/10 [00:11<00:25,  3.68s/it]\n",
      " 40%|████      | 4/10 [00:15<00:22,  3.73s/it]\n",
      " 50%|█████     | 5/10 [00:18<00:18,  3.70s/it]\n",
      " 60%|██████    | 6/10 [00:22<00:14,  3.69s/it]\n",
      " 70%|███████   | 7/10 [00:26<00:11,  3.76s/it]\n",
      " 80%|████████  | 8/10 [00:29<00:07,  3.72s/it]\n",
      " 90%|█████████ | 9/10 [00:39<00:05,  5.49s/it]\n",
      "100%|██████████| 10/10 [00:55<00:00,  8.75s/it]\n",
      "100%|██████████| 10/10 [00:55<00:00,  5.53s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/shortest_path.py --model davinci-002 --mode hard --prompt CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = COT  \n",
    "##### Task = Hamilton Path   \n",
    "##### Difficulty = Easy Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:04<00:28,  4.09s/it]\n",
      " 25%|██▌       | 2/8 [00:07<00:22,  3.73s/it]\n",
      " 38%|███▊      | 3/8 [00:11<00:18,  3.73s/it]\n",
      " 50%|█████     | 4/8 [00:14<00:14,  3.65s/it]\n",
      " 62%|██████▎   | 5/8 [00:18<00:10,  3.60s/it]\n",
      " 75%|███████▌  | 6/8 [00:21<00:07,  3.55s/it]\n",
      " 88%|████████▊ | 7/8 [00:25<00:03,  3.50s/it]\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.38s/it]\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.54s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/hamilton.py --model davinci-002 --mode easy --prompt CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:03<00:34,  3.87s/it]\n",
      " 20%|██        | 2/10 [00:07<00:29,  3.65s/it]\n",
      " 30%|███       | 3/10 [00:10<00:25,  3.60s/it]\n",
      " 40%|████      | 4/10 [00:14<00:21,  3.58s/it]\n",
      " 50%|█████     | 5/10 [00:18<00:17,  3.57s/it]\n",
      " 60%|██████    | 6/10 [00:21<00:14,  3.59s/it]\n",
      " 70%|███████   | 7/10 [00:25<00:10,  3.59s/it]\n",
      " 80%|████████  | 8/10 [00:28<00:07,  3.61s/it]\n",
      " 90%|█████████ | 9/10 [00:40<00:05,  6.00s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  8.51s/it]\n",
      "100%|██████████| 10/10 [00:54<00:00,  5.43s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/hamilton.py --model davinci-002 --mode hard --prompt CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build-a-Graph**  \n",
    "\n",
    "Now we test the Build-a-Graph prompting method below for the different tasks and difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = BAG\n",
    "##### Task = Cycles \n",
    "##### Difficulty = Easy Medium Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:03<00:25,  3.68s/it]\n",
      " 25%|██▌       | 2/8 [00:06<00:20,  3.40s/it]\n",
      " 38%|███▊      | 3/8 [00:10<00:16,  3.30s/it]\n",
      " 50%|█████     | 4/8 [00:13<00:13,  3.27s/it]\n",
      " 62%|██████▎   | 5/8 [00:16<00:09,  3.30s/it]\n",
      " 75%|███████▌  | 6/8 [00:20<00:06,  3.34s/it]\n",
      " 88%|████████▊ | 7/8 [00:23<00:03,  3.31s/it]\n",
      "100%|██████████| 8/8 [00:26<00:00,  3.21s/it]\n",
      "100%|██████████| 8/8 [00:26<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode easy --prompt Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\n",
      "  3%|▎         | 1/30 [00:03<01:42,  3.55s/it]\n",
      "  7%|▋         | 2/30 [00:06<01:34,  3.38s/it]\n",
      " 10%|█         | 3/30 [00:10<01:29,  3.32s/it]\n",
      " 13%|█▎        | 4/30 [00:13<01:26,  3.31s/it]\n",
      " 17%|█▋        | 5/30 [00:16<01:23,  3.34s/it]\n",
      " 20%|██        | 6/30 [00:20<01:19,  3.33s/it]\n",
      " 23%|██▎       | 7/30 [00:23<01:16,  3.32s/it]\n",
      " 27%|██▋       | 8/30 [00:26<01:12,  3.28s/it]\n",
      " 30%|███       | 9/30 [00:29<01:08,  3.26s/it]\n",
      " 33%|███▎      | 10/30 [00:33<01:05,  3.27s/it]\n",
      " 37%|███▋      | 11/30 [00:36<01:01,  3.26s/it]\n",
      " 40%|████      | 12/30 [00:39<00:58,  3.25s/it]\n",
      " 43%|████▎     | 13/30 [00:42<00:54,  3.21s/it]\n",
      " 47%|████▋     | 14/30 [00:45<00:51,  3.22s/it]\n",
      " 50%|█████     | 15/30 [00:49<00:48,  3.23s/it]\n",
      " 53%|█████▎    | 16/30 [00:52<00:45,  3.25s/it]\n",
      " 57%|█████▋    | 17/30 [00:56<00:44,  3.41s/it]\n",
      " 60%|██████    | 18/30 [00:59<00:40,  3.38s/it]\n",
      " 63%|██████▎   | 19/30 [01:02<00:36,  3.35s/it]\n",
      " 67%|██████▋   | 20/30 [01:06<00:33,  3.34s/it]\n",
      " 70%|███████   | 21/30 [01:09<00:30,  3.33s/it]\n",
      " 73%|███████▎  | 22/30 [01:12<00:26,  3.32s/it]\n",
      " 77%|███████▋  | 23/30 [01:16<00:23,  3.32s/it]\n",
      " 80%|████████  | 24/30 [01:19<00:19,  3.33s/it]\n",
      " 83%|████████▎ | 25/30 [01:22<00:16,  3.32s/it]\n",
      " 87%|████████▋ | 26/30 [01:26<00:13,  3.32s/it]\n",
      " 90%|█████████ | 27/30 [01:29<00:10,  3.34s/it]\n",
      " 93%|█████████▎| 28/30 [01:32<00:06,  3.38s/it]\n",
      " 97%|█████████▋| 29/30 [01:36<00:03,  3.38s/it]\n",
      "100%|██████████| 30/30 [01:39<00:00,  3.36s/it]\n",
      "100%|██████████| 30/30 [01:39<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode medium --prompt Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|▌         | 1/20 [00:03<01:10,  3.71s/it]\n",
      " 10%|█         | 2/20 [00:07<01:02,  3.50s/it]\n",
      " 15%|█▌        | 3/20 [00:10<00:57,  3.40s/it]\n",
      " 20%|██        | 4/20 [00:13<00:53,  3.35s/it]\n",
      " 25%|██▌       | 5/20 [00:16<00:48,  3.26s/it]\n",
      " 30%|███       | 6/20 [00:19<00:45,  3.23s/it]\n",
      " 35%|███▌      | 7/20 [00:22<00:41,  3.19s/it]\n",
      " 40%|████      | 8/20 [00:26<00:38,  3.18s/it]\n",
      " 45%|████▌     | 9/20 [00:29<00:34,  3.17s/it]\n",
      " 50%|█████     | 10/20 [00:32<00:31,  3.17s/it]\n",
      " 55%|█████▌    | 11/20 [00:35<00:28,  3.20s/it]\n",
      " 60%|██████    | 12/20 [00:39<00:25,  3.23s/it]\n",
      " 65%|██████▌   | 13/20 [00:42<00:22,  3.27s/it]\n",
      " 70%|███████   | 14/20 [00:45<00:19,  3.26s/it]\n",
      " 75%|███████▌  | 15/20 [00:49<00:16,  3.30s/it]\n",
      " 80%|████████  | 16/20 [00:52<00:13,  3.29s/it]\n",
      " 85%|████████▌ | 17/20 [00:55<00:09,  3.30s/it]\n",
      " 90%|█████████ | 18/20 [01:01<00:08,  4.02s/it]\n",
      " 95%|█████████▌| 19/20 [01:06<00:04,  4.46s/it]\n",
      "100%|██████████| 20/20 [01:12<00:00,  4.89s/it]\n",
      "100%|██████████| 20/20 [01:12<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode hard --prompt Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = BAG  \n",
    "##### Task = Shortest Path  \n",
    "##### Difficulty = Easy Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\n",
      " 11%|█         | 1/9 [00:04<00:34,  4.31s/it]\n",
      " 22%|██▏       | 2/9 [00:08<00:29,  4.24s/it]\n",
      " 33%|███▎      | 3/9 [00:11<00:23,  3.90s/it]\n",
      " 44%|████▍     | 4/9 [00:15<00:18,  3.76s/it]\n",
      " 56%|█████▌    | 5/9 [00:19<00:14,  3.69s/it]\n",
      " 67%|██████▋   | 6/9 [00:22<00:11,  3.67s/it]\n",
      " 78%|███████▊  | 7/9 [00:26<00:07,  3.64s/it]\n",
      " 89%|████████▉ | 8/9 [00:29<00:03,  3.62s/it]\n",
      "100%|██████████| 9/9 [00:40<00:00,  5.96s/it]\n",
      "100%|██████████| 9/9 [00:40<00:00,  4.55s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/shortest_path.py --model davinci-002 --mode easy --prompt Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:03<00:34,  3.79s/it]\n",
      " 20%|██        | 2/10 [00:07<00:29,  3.67s/it]\n",
      " 30%|███       | 3/10 [00:10<00:25,  3.63s/it]\n",
      " 40%|████      | 4/10 [00:14<00:21,  3.59s/it]\n",
      " 50%|█████     | 5/10 [00:18<00:17,  3.60s/it]\n",
      " 60%|██████    | 6/10 [00:21<00:14,  3.67s/it]\n",
      " 70%|███████   | 7/10 [00:29<00:14,  4.86s/it]\n",
      " 80%|████████  | 8/10 [00:40<00:13,  6.82s/it]\n",
      " 90%|█████████ | 9/10 [00:56<00:09,  9.80s/it]\n",
      "100%|██████████| 10/10 [01:07<00:00, 10.06s/it]\n",
      "100%|██████████| 10/10 [01:07<00:00,  6.72s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/shortest_path.py --model davinci-002 --mode hard --prompt Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = BAG  \n",
    "##### Task = Hamilton Path  \n",
    "##### Difficulty = Easy Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:03<00:27,  3.92s/it]\n",
      " 25%|██▌       | 2/8 [00:07<00:22,  3.73s/it]\n",
      " 38%|███▊      | 3/8 [00:11<00:18,  3.68s/it]\n",
      " 50%|█████     | 4/8 [00:14<00:14,  3.61s/it]\n",
      " 62%|██████▎   | 5/8 [00:18<00:10,  3.55s/it]\n",
      " 75%|███████▌  | 6/8 [00:21<00:07,  3.53s/it]\n",
      " 88%|████████▊ | 7/8 [00:25<00:03,  3.52s/it]\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.39s/it]\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/hamilton.py --model davinci-002 --mode easy --prompt Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:03<00:35,  3.93s/it]\n",
      " 20%|██        | 2/10 [00:07<00:28,  3.58s/it]\n",
      " 30%|███       | 3/10 [00:10<00:25,  3.58s/it]\n",
      " 40%|████      | 4/10 [00:14<00:21,  3.53s/it]\n",
      " 50%|█████     | 5/10 [00:21<00:23,  4.72s/it]\n",
      " 60%|██████    | 6/10 [00:38<00:36,  9.07s/it]\n",
      " 70%|███████   | 7/10 [00:42<00:21,  7.31s/it]\n",
      " 80%|████████  | 8/10 [00:58<00:20, 10.09s/it]\n",
      " 90%|█████████ | 9/10 [01:08<00:10, 10.02s/it]\n",
      "100%|██████████| 10/10 [01:20<00:00, 10.61s/it]\n",
      "100%|██████████| 10/10 [01:20<00:00,  8.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/hamilton.py --model davinci-002 --mode hard --prompt Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Algorithmic**  \n",
    "\n",
    "Now we test the Algorithmic prompting method below for the different tasks and difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = Algorithm \n",
    "##### Task = Cycles \n",
    "##### Difficulty = Easy Medium Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:03<00:25,  3.68s/it]\n",
      " 25%|██▌       | 2/8 [00:06<00:20,  3.39s/it]\n",
      " 38%|███▊      | 3/8 [00:10<00:16,  3.31s/it]\n",
      " 50%|█████     | 4/8 [00:13<00:13,  3.29s/it]\n",
      " 62%|██████▎   | 5/8 [00:16<00:09,  3.23s/it]\n",
      " 75%|███████▌  | 6/8 [00:19<00:06,  3.25s/it]\n",
      " 88%|████████▊ | 7/8 [00:22<00:03,  3.22s/it]\n",
      "100%|██████████| 8/8 [00:26<00:00,  3.35s/it]\n",
      "100%|██████████| 8/8 [00:26<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode easy --prompt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\n",
      "  3%|▎         | 1/30 [00:03<01:42,  3.53s/it]\n",
      "  7%|▋         | 2/30 [00:06<01:34,  3.36s/it]\n",
      " 10%|█         | 3/30 [00:10<01:29,  3.30s/it]\n",
      " 13%|█▎        | 4/30 [00:13<01:25,  3.28s/it]\n",
      " 17%|█▋        | 5/30 [00:16<01:21,  3.27s/it]\n",
      " 20%|██        | 6/30 [00:19<01:18,  3.28s/it]\n",
      " 23%|██▎       | 7/30 [00:23<01:14,  3.26s/it]\n",
      " 27%|██▋       | 8/30 [00:26<01:12,  3.28s/it]\n",
      " 30%|███       | 9/30 [00:29<01:09,  3.29s/it]\n",
      " 33%|███▎      | 10/30 [00:32<01:05,  3.27s/it]\n",
      " 37%|███▋      | 11/30 [00:36<01:01,  3.26s/it]\n",
      " 40%|████      | 12/30 [00:39<00:58,  3.24s/it]\n",
      " 43%|████▎     | 13/30 [00:42<00:55,  3.27s/it]\n",
      " 47%|████▋     | 14/30 [00:45<00:52,  3.26s/it]\n",
      " 50%|█████     | 15/30 [00:49<00:48,  3.25s/it]\n",
      " 53%|█████▎    | 16/30 [00:52<00:45,  3.26s/it]\n",
      " 57%|█████▋    | 17/30 [00:55<00:42,  3.26s/it]\n",
      " 60%|██████    | 18/30 [00:58<00:39,  3.26s/it]\n",
      " 63%|██████▎   | 19/30 [01:02<00:35,  3.24s/it]\n",
      " 67%|██████▋   | 20/30 [01:05<00:33,  3.31s/it]\n",
      " 70%|███████   | 21/30 [01:08<00:29,  3.33s/it]\n",
      " 73%|███████▎  | 22/30 [01:12<00:26,  3.31s/it]\n",
      " 77%|███████▋  | 23/30 [01:15<00:23,  3.38s/it]\n",
      " 80%|████████  | 24/30 [01:19<00:20,  3.39s/it]\n",
      " 83%|████████▎ | 25/30 [01:22<00:17,  3.42s/it]\n",
      " 87%|████████▋ | 26/30 [01:26<00:13,  3.40s/it]\n",
      " 90%|█████████ | 27/30 [01:29<00:10,  3.39s/it]\n",
      " 93%|█████████▎| 28/30 [01:32<00:06,  3.41s/it]\n",
      " 97%|█████████▋| 29/30 [01:36<00:03,  3.38s/it]\n",
      "100%|██████████| 30/30 [01:39<00:00,  3.37s/it]\n",
      "100%|██████████| 30/30 [01:39<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode medium --prompt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  5%|▌         | 1/20 [00:03<01:08,  3.58s/it]\n",
      " 10%|█         | 2/20 [00:06<00:59,  3.32s/it]\n",
      " 15%|█▌        | 3/20 [00:10<01:00,  3.59s/it]\n",
      " 20%|██        | 4/20 [00:14<00:56,  3.55s/it]\n",
      " 25%|██▌       | 5/20 [00:17<00:52,  3.47s/it]\n",
      " 30%|███       | 6/20 [00:20<00:48,  3.50s/it]\n",
      " 35%|███▌      | 7/20 [00:24<00:45,  3.50s/it]\n",
      " 40%|████      | 8/20 [00:28<00:42,  3.54s/it]\n",
      " 45%|████▌     | 9/20 [00:31<00:37,  3.45s/it]\n",
      " 50%|█████     | 10/20 [00:34<00:33,  3.36s/it]\n",
      " 55%|█████▌    | 11/20 [00:38<00:30,  3.42s/it]\n",
      " 60%|██████    | 12/20 [00:41<00:26,  3.37s/it]\n",
      " 65%|██████▌   | 13/20 [00:44<00:23,  3.34s/it]\n",
      " 70%|███████   | 14/20 [00:48<00:20,  3.43s/it]\n",
      " 75%|███████▌  | 15/20 [00:51<00:16,  3.39s/it]\n",
      " 80%|████████  | 16/20 [00:55<00:13,  3.43s/it]\n",
      " 85%|████████▌ | 17/20 [00:58<00:10,  3.50s/it]\n",
      " 90%|█████████ | 18/20 [01:02<00:07,  3.57s/it]\n",
      " 95%|█████████▌| 19/20 [01:06<00:03,  3.61s/it]\n",
      "100%|██████████| 20/20 [01:10<00:00,  3.68s/it]\n",
      "100%|██████████| 20/20 [01:10<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/cycle.py --model davinci-002 --mode hard --prompt Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = Algorithm  \n",
    "##### Task = Shortest path  \n",
    "##### Difficulty = Easy Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]\n",
      " 11%|█         | 1/9 [00:04<00:34,  4.33s/it]\n",
      " 22%|██▏       | 2/9 [00:07<00:27,  3.87s/it]\n",
      " 33%|███▎      | 3/9 [00:11<00:22,  3.81s/it]\n",
      " 44%|████▍     | 4/9 [00:15<00:18,  3.70s/it]\n",
      " 56%|█████▌    | 5/9 [00:19<00:15,  3.77s/it]\n",
      " 67%|██████▋   | 6/9 [00:22<00:11,  3.76s/it]\n",
      " 78%|███████▊  | 7/9 [00:26<00:07,  3.68s/it]\n",
      " 89%|████████▉ | 8/9 [00:29<00:03,  3.65s/it]\n",
      "100%|██████████| 9/9 [00:45<00:00,  7.30s/it]\n",
      "100%|██████████| 9/9 [00:45<00:00,  5.02s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/shortest_path.py --model davinci-002 --mode easy --prompt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:03<00:34,  3.86s/it]\n",
      " 20%|██        | 2/10 [00:07<00:31,  3.97s/it]\n",
      " 30%|███       | 3/10 [00:17<00:45,  6.55s/it]\n",
      " 40%|████      | 4/10 [00:33<01:01, 10.24s/it]\n",
      " 50%|█████     | 5/10 [00:40<00:44,  8.98s/it]\n",
      " 60%|██████    | 6/10 [00:51<00:39,  9.84s/it]\n",
      " 70%|███████   | 7/10 [01:04<00:32, 10.77s/it]\n",
      " 80%|████████  | 8/10 [01:13<00:20, 10.35s/it]\n",
      " 90%|█████████ | 9/10 [01:29<00:11, 11.92s/it]\n",
      "100%|██████████| 10/10 [01:39<00:00, 11.56s/it]\n",
      "100%|██████████| 10/10 [01:39<00:00,  9.99s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/shortest_path.py --model davinci-002 --mode hard --prompt Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method = Algorithm \n",
    "##### Task = Hamilton Path \n",
    "##### Difficulty = Easy Hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n",
      " 12%|█▎        | 1/8 [00:03<00:27,  3.96s/it]\n",
      " 25%|██▌       | 2/8 [00:07<00:21,  3.63s/it]\n",
      " 38%|███▊      | 3/8 [00:10<00:17,  3.56s/it]\n",
      " 50%|█████     | 4/8 [00:14<00:14,  3.53s/it]\n",
      " 62%|██████▎   | 5/8 [00:17<00:10,  3.54s/it]\n",
      " 75%|███████▌  | 6/8 [00:21<00:07,  3.59s/it]\n",
      " 88%|████████▊ | 7/8 [00:25<00:03,  3.55s/it]\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.44s/it]\n",
      "100%|██████████| 8/8 [00:28<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/hamilton.py --model davinci-002 --mode easy --prompt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\n",
      " 10%|█         | 1/10 [00:04<00:37,  4.17s/it]\n",
      " 20%|██        | 2/10 [00:07<00:31,  3.88s/it]\n",
      " 30%|███       | 3/10 [00:11<00:26,  3.75s/it]\n",
      " 40%|████      | 4/10 [00:14<00:21,  3.65s/it]\n",
      " 50%|█████     | 5/10 [00:28<00:35,  7.13s/it]\n",
      " 60%|██████    | 6/10 [00:38<00:32,  8.17s/it]\n",
      " 70%|███████   | 7/10 [00:54<00:32, 10.86s/it]\n",
      " 80%|████████  | 8/10 [01:05<00:21, 10.69s/it]\n",
      " 90%|█████████ | 9/10 [01:27<00:14, 14.40s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00, 11.09s/it]\n",
      "100%|██████████| 10/10 [01:31<00:00,  9.14s/it]\n"
     ]
    }
   ],
   "source": [
    "!python evaluation/hamilton.py --model davinci-002 --mode hard --prompt Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Prompt Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is 1 example of a test we ran way above, the below 3 output files are created and stored in the log folder for each evaluation run we do. Please refer to the logs folder in the repo to manually inspect individual test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer data shape: (150,)\n",
      "Result data shape: (150,)\n",
      "Prompt text: In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 5, and the edges are: (3,4) (3,5) (1,0) (2,5) (2,0)\n",
      "Q: Is there a cycle in this graph?\n",
      "A: No, there is no cycle in this graph.\n",
      "\n",
      "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 5, and the edges are: (3,5) (1,0) (3,0) (3,4) (4,1) (2,3)\n",
      "Q: Is there a cycle in this graph?\n",
      "A: The edges (3,0), (1,0), (4,1), (3,4) form a cycle, so yes, there is a cycle in this graph.\n",
      "\n",
      "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 7, and the edges are: (7,1) (5,7) (0,2) (3,7) (2,4) (5,6) (2,6)\n",
      "Q: Is there a cycle in this graph?\n",
      "A: No, there is no cycle in this graph.\n",
      "\n",
      "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 5, and the edges are: (0,1) (2,6) (2,0) (5,3) (1,4) (4,7) (3,7) (6,4)\n",
      "Q: Is there a cycle in this graph?\n",
      "A: The edges (0,1), (1,4), (6,4), (2,6), (2,0) form a cycle, so yes, there is a cycle in this graph.\n",
      "\n",
      "\n",
      "In an undirected graph, (i,j) means that node i and node j are connected with an undirected edge.\n",
      "The nodes are numbered from 0 to 9, and the edges are: (0,2) (3,7) (5,3) (0,1) (3,0) (9,0) (8,4) (2,8) (1,7) (6,2) (5,0) (4,5)\n",
      "Q: Is there a cycle in this graph?\n",
      "A:\n",
      "Acc: 76/150\n",
      "Namespace(model='davinci-002', mode='easy', prompt='CoT', T=0, token=400, SC=0, SC_num=5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data from the .npy files\n",
    "answer_data = np.load(\"log/cycle/davinci-002-easy-20240422---12-23-CoT/answer.npy\")\n",
    "res_data = np.load(\"log/cycle/davinci-002-easy-20240422---12-23-CoT/res.npy\")\n",
    "\n",
    "# Load the prompt from the .txt file\n",
    "with open(\"log/cycle/davinci-002-easy-20240422---12-23-CoT/prompt.txt\", \"r\") as prompt_file:\n",
    "    prompt_text = prompt_file.read()\n",
    "\n",
    "print(\"Answer data shape:\", answer_data.shape)\n",
    "print(\"Result data shape:\", res_data.shape)\n",
    "print(\"Prompt text:\", prompt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating Accuracy Results Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we compile all the results from the evaluation we did above. See figure A below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile regular expressions for efficiency\n",
    "mode_pattern = re.compile(r\"mode='(\\w+)'\")\n",
    "acc_pattern = re.compile(r\"Acc: (\\d+)/(\\d+)\")\n",
    "\n",
    "def extract_info_from_prompt_file(prompt_file_path):\n",
    "    with open(prompt_file_path, \"r\") as prompt_file:\n",
    "        prompt_text = prompt_file.read()\n",
    "\n",
    "    mode_match = mode_pattern.search(prompt_text)\n",
    "    acc_match = acc_pattern.search(prompt_text)\n",
    "\n",
    "    mode = mode_match.group(1) if mode_match else \"Unknown\"\n",
    "    acc = f\"{acc_match.group(1)}/{acc_match.group(2)}\" if acc_match else \"Unknown\"\n",
    "        \n",
    "    return mode, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths_dict = {\n",
    "    \"Cycles\": [\n",
    "        \"log/cycle/davinci-002-easy-20240422---13-10-Algorithm/prompt.txt\",\n",
    "        \"log/cycle/davinci-002-medium-20240422---13-14-Algorithm/prompt.txt\",\n",
    "        \"log/cycle/davinci-002-hard-20240422---13-21-Algorithm/prompt.txt\"\n",
    "    ],\n",
    "    \"Shortest Path\": [\n",
    "        \"log/shortest_path/davinci-002-easy-20240422---13-24-Algorithm/prompt.txt\",\n",
    "        \"log/shortest_path/davinci-002-hard-20240422---13-26-Algorithm/prompt.txt\"\n",
    "    ],\n",
    "    \"Hamilton Path\": [\n",
    "        \"log/hamilton/davinci-002-easy-20240422---13-27-Algorithm/prompt.txt\",\n",
    "        \"log/hamilton/davinci-002-hard-20240422---13-29-Algorithm/prompt.txt\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "result_df1 = pd.DataFrame()\n",
    "\n",
    "for category, paths in file_paths_dict.items():\n",
    "    data_dict = {\"Difficulty\": [], \"Accuracy\": []}\n",
    "    for file_path in paths:\n",
    "        mode, acc = extract_info_from_prompt_file(file_path)\n",
    "        data_dict[\"Difficulty\"].append(mode)\n",
    "        data_dict[\"Accuracy\"].append(acc)\n",
    "    temp_df = pd.DataFrame(data_dict)\n",
    "    temp_df[\"Category\"] = category \n",
    "    result_df1 = pd.concat([result_df1, temp_df], ignore_index=True)\n",
    "\n",
    "result_df1 = result_df1.pivot_table(index=\"Difficulty\", columns=\"Category\", values=\"Accuracy\", aggfunc=lambda x: ' '.join(x))\n",
    "chart1 = result_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfile_paths = {\n",
    "    \"Cycles\": [\n",
    "        \"log/cycle/davinci-002-easy-20240422---12-23-CoT/prompt.txt\",\n",
    "        \"log/cycle/davinci-002-medium-20240422---12-27-CoT/prompt.txt\",\n",
    "        \"log/cycle/davinci-002-hard-20240422---12-28-CoT/prompt.txt\"\n",
    "    ],\n",
    "    \"Shortest Path\": [\n",
    "        \"log/shortest_path/davinci-002-easy-20240422---12-34-CoT/prompt.txt\",\n",
    "        \"log/shortest_path/davinci-002-hard-20240422---12-36-CoT/prompt.txt\"\n",
    "    ],\n",
    "    \"Hamilton Path\": [\n",
    "        \"log/hamilton/davinci-002-easy-20240422---12-42-CoT/prompt.txt\",\n",
    "        \"log/hamilton/davinci-002-hard-20240422---12-43-CoT/prompt.txt\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "result_df2 = pd.DataFrame()\n",
    "\n",
    "for category, paths in cfile_paths.items():\n",
    "    data_dict = {\"Difficulty\": [], \"Accuracy\": []}\n",
    "    for file_path in paths:\n",
    "        mode, acc = extract_info_from_prompt_file(file_path)\n",
    "        data_dict[\"Difficulty\"].append(mode)\n",
    "        data_dict[\"Accuracy\"].append(acc)\n",
    "    temp_df = pd.DataFrame(data_dict)\n",
    "    temp_df[\"Category\"] = category  \n",
    "    result_df2 = pd.concat([result_df2, temp_df], ignore_index=True)\n",
    "\n",
    "result_df2 = result_df2.pivot_table(index=\"Difficulty\", columns=\"Category\", values=\"Accuracy\", aggfunc=lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile_paths = {\n",
    "    \"Cycles\": [\n",
    "        \"log/cycle/davinci-002-easy-20240422---12-46-Instruct/prompt.txt\",\n",
    "        \"log/cycle/davinci-002-medium-20240422---12-48-Instruct/prompt.txt\",\n",
    "        \"log/cycle/davinci-002-hard-20240422---12-49-Instruct/prompt.txt\"\n",
    "    ],\n",
    "    \"Shortest Path\": [\n",
    "        \"log/shortest_path/davinci-002-easy-20240422---13-01-Instruct/prompt.txt\",\n",
    "        \"log/shortest_path/davinci-002-hard-20240422---13-03-Instruct/prompt.txt\"\n",
    "    ],\n",
    "    \"Hamilton Path\": [\n",
    "        \"log/hamilton/davinci-002-easy-20240422---13-05-Instruct/prompt.txt\",\n",
    "        \"log/hamilton/davinci-002-hard-20240422---13-07-Instruct/prompt.txt\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "result_df3 = pd.DataFrame()\n",
    "\n",
    "for category, paths in ifile_paths.items():\n",
    "    data_dict = {\"Difficulty\": [], \"Accuracy\": []}\n",
    "    for file_path in paths:\n",
    "        mode, acc = extract_info_from_prompt_file(file_path)\n",
    "        data_dict[\"Difficulty\"].append(mode)\n",
    "        data_dict[\"Accuracy\"].append(acc)\n",
    "    temp_df = pd.DataFrame(data_dict)\n",
    "    temp_df[\"Category\"] = category  \n",
    "    result_df3 = pd.concat([result_df3, temp_df], ignore_index=True)\n",
    "\n",
    "result_df3 = result_df3.pivot_table(index=\"Difficulty\", columns=\"Category\", values=\"Accuracy\", aggfunc=lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought\n",
      "╒══════════════╤══════════╤═════════════════╤═════════════════╕\n",
      "│ Difficulty   │ Cycles   │ Hamilton Path   │ Shortest Path   │\n",
      "╞══════════════╪══════════╪═════════════════╪═════════════════╡\n",
      "│ easy         │ 75/150   │ 5/150           │ 8/180           │\n",
      "├──────────────┼──────────┼─────────────────┼─────────────────┤\n",
      "│ hard         │ 140/400  │ 0/200           │ 0/200           │\n",
      "├──────────────┼──────────┼─────────────────┼─────────────────┤\n",
      "│ medium       │ 248/600  │ nan             │ nan             │\n",
      "╘══════════════╧══════════╧═════════════════╧═════════════════╛\n",
      "Build-a-Graph\n",
      "╒══════════════╤══════════╤═════════════════╤═════════════════╕\n",
      "│ Difficulty   │ Cycles   │ Hamilton Path   │ Shortest Path   │\n",
      "╞══════════════╪══════════╪═════════════════╪═════════════════╡\n",
      "│ easy         │ 76/150   │ 9/150           │ 7/180           │\n",
      "├──────────────┼──────────┼─────────────────┼─────────────────┤\n",
      "│ hard         │ 159/400  │ 0/200           │ 1/200           │\n",
      "├──────────────┼──────────┼─────────────────┼─────────────────┤\n",
      "│ medium       │ 264/600  │ nan             │ nan             │\n",
      "╘══════════════╧══════════╧═════════════════╧═════════════════╛\n",
      "Algorithmic\n",
      "╒══════════════╤══════════╤═════════════════╤═════════════════╕\n",
      "│ Difficulty   │ Cycles   │ Hamilton Path   │ Shortest Path   │\n",
      "╞══════════════╪══════════╪═════════════════╪═════════════════╡\n",
      "│ easy         │ 74/150   │ 6/150           │ 7/180           │\n",
      "├──────────────┼──────────┼─────────────────┼─────────────────┤\n",
      "│ hard         │ 161/400  │ 0/200           │ 0/200           │\n",
      "├──────────────┼──────────┼─────────────────┼─────────────────┤\n",
      "│ medium       │ 264/600  │ nan             │ nan             │\n",
      "╘══════════════╧══════════╧═════════════════╧═════════════════╛\n"
     ]
    }
   ],
   "source": [
    "print(\"Chain-of-Thought\")\n",
    "print(tabulate(chart1.reset_index(), headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "\n",
    "print(\"Build-a-Graph\")\n",
    "print(tabulate(result_df2.reset_index(), headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "\n",
    "print(\"Algorithmic\")\n",
    "print(tabulate(result_df3.reset_index(), headers='keys', tablefmt='fancy_grid', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure A: Table with Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusion and Future Direction**  \n",
    "\n",
    "## **Results**\n",
    "In our results, we observed distinct variations in performance across different prompting methods applied to graph reasoning tasks from Figure A above. For the Cycles task, the Build-a-Graph (BAG) and Algorithmic methods showed notable improvements over the Chain-of-Thought (CoT) method. Specifically, BAG and Algorithmic outperformed CoT by 6.45% at the medium difficulty level, and 13.57% and 15%, respectively, at the hard difficulty level. In the Hamilton Path task, BAG significantly enhanced performance by 80% over CoT at the easy level, whereas Algorithmic showed a more modest improvement of 20.12%. Notably, for the Shortest Path task, BAG and Alg did not consistently outperform CoT, especially at the easy level where both showed a 12.39% decrease in performance compared to CoT. However, BAG alone showed marginal improvement in the hard level with a 0.5% increase, the only method to record a correct answer in this category. These results underscore the potential of specialized prompting techniques like BAG and Alg in improving the reasoning capabilities of language models on complex graph-based tasks, particularly in scenarios requiring higher-order thinking and structured problem-solving approaches.\n",
    "\n",
    "In comparing the results of our study to the original investigation into the efficacy of prompting methods on graph-based tasks, we observed some similarties. The original study reported a performance improvement ranging from 3.07% to 16.85% for easier tasks like cycles and shortest paths when utilizing Build-a-Graph (BAG) and Algorithmic (Alg) prompting methods. Our findings align with these observations, particularly for the Cycles task, where we noted improvements up to 15% in the hard category and 6.45% in the medium category. Similarly, for the Hamilton Path task, we found a substantial increase of up to 80% at the easy level using BAG, significantly exceeding the gains reported in the original study.\n",
    "\n",
    "## **Learnings**  \n",
    "Learnt the importance of natural language prompting when working with LLMs. Learnt about the proposed BAG and Algorithmic methods, and how they improve performance in certain graph reasoning tasks. We also learnt that Diminishing returns of advanced prompting methods with increasing task complexity.\n",
    "\n",
    "\n",
    "## **Limitations/Future Direction** \n",
    "\n",
    "One of the primary limitations identified in our study, mirroring concerns from the original research, is the diminishing returns of advanced prompting techniques as task complexity increases. Furthermore we also had a limitation for financial constraints, as the use of LLM APIs involves considerable costs, which restricted the breadth and depth of experiments we could conduct.Future research should focus on developing more sophisticated prompting techniques and making LLMs robust against misleading correlations in problem settings. Additionally, expanding the NLGraph benchmark and integrating interdisciplinary approaches could enhance the models' effectiveness and applicability in solving complex graph-based problems in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "- Chen, Zhikai, et al. Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. arXiv:2307.03393, arXiv, 16 Jan. 2024. arXiv.org, http://arxiv.org/abs/2307.03393.\n",
    "- Huang, Wenlong, et al. “Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.” Proceedings of the 39th International Conference on Machine Learning, PMLR, 2022, pp. 9118–47. proceedings.mlr.press, https://proceedings.mlr.press/v162/huang22a.html.\n",
    "- Jin, Bowen, et al. Large Language Models on Graphs: A Comprehensive Survey. 2, arXiv:2312.02783, arXiv, 1 Feb. 2024. arXiv.org, http://arxiv.org/abs/2312.02783.\n",
    "- Wang, Heng, et al. Can Language Models Solve Graph Problems in Natural Language? arXiv:2305.10037, arXiv, 5 Jan. 2023. arXiv.org, http://arxiv.org/abs/2305.10037.\n",
    "- Zhang, Jiawei. Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. arXiv:2304.11116, arXiv, 11 May 2023. arXiv.org, http://arxiv.org/abs/2304.11116."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
